---
title : 装载数据
---

## 主要函数

parallelize ;  textFile  ; wholetextFiles

>> 装载数据的目的为了生成RDD。

做任何Spark开发前，都先要对spark核心类SparkContext有一个清晰的认识,以下链接来自spark官网的说明。
[SparkContext](core-class-pysaprk/pyspark.SparkContext.md)                    

Spakcontext表示与Spark群集的连接,可用于在该群集上创建RDD和广播变量。所以在进行任何Spark操作前都是需要SparkContext对象

class pyspark.SparkContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)
-   Master - 它是连接到的集群的URL。
-   appName - 工作名称。
-   sparkHome - Spark安装目录。
-   pyFiles - 要发送到集群并添加到PYTHONPATH的.zip或.py文件。
-   environment - 工作节点环境变量。
-   batchSize - 表示为单个Java对象的Python对象的数量。设置1以禁用批处理，设置0以根据对象大小自动选择批处理大小，或设置为-1以使用无限批处理大小。
-   serializer - RDD序列化器。
-   conf - L {SparkConf}的一个对象，用于设置所有Spark属性。
-   gateway- 使用现有网关和JVM，否则初始化新JVM。
-   JSC - JavaSparkContext实例。
-   profiler_cls - 用于进行性能分析的一类自定义Profiler（默认为pyspark.profiler.BasicProfiler

### **parallelize(c,numSlices=None)**

~~~python

~~~

**除了开发原型和测试时，这种方式用得并不多，毕竟这种方式需要把整个数据集先放在一台机器的内存中**

### textFile

从外部读完数据，可以在不同的数据源上进行。


### wholetextFiles

## 总结


